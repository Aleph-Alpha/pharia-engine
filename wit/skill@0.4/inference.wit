interface inference {
    /// Better understand the source of a completion, specifically on how much each section of a prompt impacts each token of the completion.
    explain: func(request: list<explanation-request>) -> list<list<text-score>>;

    /// A score for a text segment.
    record text-score {
        /// The start index of the text segment w.r.t. to characters in the prompt.
        start: u32,
        /// The length of the text segment w.r.t. to characters in the prompt.
        length: u32,
        /// The score of the text segment, higher means more relevant.
        score: f64,
    }

    /// At which granularity should the target be explained in terms of the prompt.
    /// If you choose, for example, [`granularity.sentence`] then we report the importance score of each
    /// sentence in the prompt towards generating the target output.
    /// The default is [`granularity.auto`] which means we will try to find the granularity that
    /// brings you closest to around 30 explanations. For large prompts, this would likely
    /// be sentences. For short prompts this might be individual words or even tokens.
    enum granularity {
        /// Let the system decide which granularity is most suitable for the given input.
        auto,
        word,
        sentence,
        paragraph,
    }

    record explanation-request {
        /// The prompt that typically was the input of a previous completion request
        prompt: string,
        /// The target string that should be explained. The influence of individual parts
        /// of the prompt for generating this target string will be indicated in the response.
        target: string,
        /// The model to use for the explanation.
        model: string,
        /// The granularity of the explanation.
        granularity: granularity,
    }

    /// The reason the model finished generating
    enum finish-reason {
        /// The model hit a natural stopping point or a provided stop sequence
        stop,
        /// The maximum number of tokens specified in the request was reached
        length,
        /// Content was omitted due to a flag from content filters
        content-filter,
    }

    record logprob {
        token: list<u8>,
        logprob: f64,
    }

    record distribution {
        /// Logarithmic probability of the token returned in the completion
        sampled: logprob,
        /// Logarithmic probabilities of the most probable tokens, filled if user has set
        /// variant `logprobs` to `top` in chat or completion request.
        top: list<logprob>,
    }

    record token-usage {
        /// Number of tokens in the prompt
        prompt: u32,
        /// Number of tokens in the generated completion
        completion: u32,
    }

    /// The result of a completion, including the text generated as well as
    /// why the model finished completing.
    record completion {
        /// The text generated by the model
        text: string,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that
        /// `completion-request.params.logprobs` has been set to `sampled` or `top`.
        logprobs: list<distribution>,
        /// Usage statistics for the completion request.
        usage: token-usage,
    }

    variant logprobs {
        /// Do not return any logprobs
        no,
        /// Return only the logprob of the tokens which have actually been sampled into the completion.
        sampled,
        /// Request between 0 and 20 tokens
        top(u8),
    }

    /// Completion request parameters
    record completion-request {
        model: string,
        prompt: string,
        params: completion-params
    }

    complete: func(requests: list<completion-request>) -> list<completion>;

    /// Completion request parameters
    record completion-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The number of possible next tokens the model will choose from.
        top-k: option<u32>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// A list of sequences that, if encountered, the API will stop generating further tokens.
        stop: list<string>,
        /// Whether to include special tokens like `<|eot_id|>` in the completion
        return-special-tokens: bool,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
        /// Echo the prompt in the completion. This may be especially helpful when log_probs is set
        /// to return logprobs for the prompt.
        echo: bool,
    }

    /// Completion request parameters
    /// Introduces support for `echo` without requiring a version bump.
    record completion-request-v2 {
        model: string,
        prompt: string,
        params: completion-params
    }

    /// Introduces support for `echo` without requiring a version bump.
    complete-v2: func(requests: list<completion-request-v2>) -> list<completion>;

    /// A chunk of a completion returned by a completion stream.
    record completion-append {
        /// A chunk of the completion text.
        text: string,
        /// Corresponding log probabilities for each token in the completion.
        logprobs: list<distribution>,
    }

    /// An event emitted by a completion stream.
    variant completion-event {
        /// A chunk of a completion returned by a completion stream.
        append(completion-append),
        /// The reason the completion stream stopped.
        end(finish-reason),
        /// The usage generated by the completion stream.
        usage(token-usage),
    }

    /// Allows for streaming completion tokens as they are generated.
    resource completion-stream {
        /// Creates a new completion-stream resource for a given completion-request.
        constructor(init: completion-request);
        /// Returns the next completion-event from the completion-stream.
        /// Will return None if the stream has finished.
        next: func() -> option<completion-event>;
    }

    record message {
        role: string,
        content: string,
    }

    record chat-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
    }

    /// The result of a chat response, including the message generated as well as
    /// why the model finished completing.
    record chat-response {
        /// The message generated by the model
        message: message,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that
        /// `chat-request.params.logprobs` has been set to `sampled` or `top`.
        logprobs: list<distribution>,
        /// Usage statistics for the chat request.
        usage: token-usage,
    }

    record chat-request {
        model: string,
        messages: list<message>,
        params: chat-params,
    }

    chat: func(requests: list<chat-request>) -> list<chat-response>;

    /// A chunk of a message generated by the model.
    record message-append {
        /// A chunk of the message content
        content: string,
        /// Corresponding log probabilities for each token in the message content
        logprobs: list<distribution>,
    }

    /// An event emitted by the chat-stream resource.
    variant chat-event {
        /// The start of a new message. It includes the role of the message.
        message-begin(string),
        /// A chunk of a message generated by the model.
        message-append(message-append),
        /// The end of a message. It includes the reason for the message end.
        message-end(finish-reason),
        /// The usage from the generated message
        usage(token-usage),
    }

    resource chat-stream {
        /// Creates a new chat-stream resource for a given chat-request.
        constructor(init: chat-request);
        /// Returns the next chat-event from the chat-stream.
        /// Will return None if the stream has finished.
        next: func() -> option<chat-event>;
    }
}