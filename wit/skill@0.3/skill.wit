package pharia:skill@0.3.0-alpha.10;

@since(version = 0.1.0)
world skill {
    @since(version = 0.1.0)
    import csi;
    @since(version = 0.1.0)
    export skill-handler;
}

@since(version = 0.1.0)
interface skill-handler {
    /// The set of errors which may be raised by functions in this interface
    @since(version = 0.1.0)
    variant error {
        internal(string),
        invalid-input(string)
    }

    @since(version = 0.1.0)
    run: func(input: list<u8>) -> result<list<u8>, error>;

    // @since(version = 0.3.0)
    record skill-metadata {
        description: option<string>,
        input-schema: list<u8>,
        output-schema: list<u8>,
    }

    // @since(version = 0.3.0)
    metadata: func() -> skill-metadata;
}

// A WASI interface dedicated to interacting with Large Language Models and other AI-related tasks.
@since(version = 0.1.0)
interface csi {
    /// The reason the model finished generating
    @since(version = 0.1.0)
    enum finish-reason {
        /// The model hit a natural stopping point or a provided stop sequence
        stop,
        /// The maximum number of tokens specified in the request was reached
        length,
        /// Content was omitted due to a flag from content filters
        content-filter,
    }

    /// The result of a completion, including the text generated as well as
    /// why the model finished completing.
    @since(version = 0.1.0)
    record completion {
        /// The text generated by the model
        text: string,
        /// The reason the model finished generating
        finish-reason: finish-reason,
    }

    /// Completion request parameters
    // @since(version = 0.3.0)
    record completion-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The number of possible next tokens the model will choose from.
        top-k: option<u32>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// A list of sequences that, if encountered, the API will stop generating further tokens.
        stop: list<string>,
        /// Whether to include special tokens like `<|eot_id|>` in the completion
        return-special-tokens: bool,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        // @since(version = 0.3.0)
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        // @since(version = 0.3.0)
        presence-penalty: option<f64>,
    }

    // @since(version = 0.3.0)
    record message {
        role: string,
        content: string,
    }

    // @since(version = 0.3.0)
    variant logprobs {
        /// Do not return any logprobs
        no,
        /// Return only the logprob of the tokens which have actually been sampled into the completion.
        sampled,
        /// Request between 0 and 20 tokens
        top(u8),
    }

    // @since(version = 0.3.0)
    record chat-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        // @since(version = 0.3.0)
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        // @since(version = 0.3.0)
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
    }

    // @since(version = 0.3.0)
    record logprob {
        token: list<u8>,
        logprob: f64,
    }

    // @since(version = 0.3.0)
    record distribution {
        // Logarithmic probability of the token returned in the completion
        sampled: logprob,
        // Logarithmic probabilities of the most probable tokens, filled if user has requested [`crate::Logprobs::Top`]
        top: list<logprob>,
    }

    /// The result of a chat response, including the message generated as well as
    /// why the model finished completing.
    /// @since(version = 0.3.0)
    record chat-response {
        /// The message generated by the model
        message: message,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that [`crate::Logprobs`] has
        /// been set to [`crate::Logprobs::Sampled`] or [`crate::Logprobs::Top`].
        logprobs: list<distribution>,
    }

    // @since(version = 0.3.0)
    record chat-request {
        model: string,
        messages: list<message>,
        params: chat-params,
    }

    // @since(version = 0.3.0)
    chat: func(requests: list<chat-request>) -> list<chat-response>;

    /// Chunking parameters
    @since(version = 0.2.1)
    record chunk-params {
        /// The name of the model the chunk is intended to be used for.
        /// This must be a known model.
        model: string,
        /// The maximum number of tokens that should be returned per chunk.
        max-tokens: u32,
    }

    // @since(version = 0.3.0)
    record chunk-request {
        text: string,
        params: chunk-params,
    }

    // @since(version = 0.3.0)
    chunk: func(request: list<chunk-request>) -> list<list<string>>;

    /// ISO 639-3
    @since(version = 0.2.2)
    enum language {
        /// Afrikaans
        afr,
        /// Arabic
        ara,
        /// Azerbaijani
        aze,
        /// Belarusian
        bel,
        /// Bengali
        ben,
        /// Bosnian
        bos,
        /// Bulgarian
        bul,
        /// Catalan
        cat,
        /// Czech
        ces,
        /// Welsh
        cym,
        /// Danish
        dan,
        /// German
        deu,
        /// Greek
        ell,
        /// English
        eng,
        /// Esperanto
        epo,
        /// Estonian
        est,
        /// Basque
        eus,
        /// Persian
        fas,
        /// Finnish
        fin,
        /// French
        fra,
        /// Irish
        gle,
        /// Gujarati
        guj,
        /// Hebrew
        heb,
        /// Hindi
        hin,
        /// Croatian
        hrv,
        /// Hungarian
        hun,
        /// Armenian
        hye,
        /// Indonesian
        ind,
        /// Icelandic
        isl,
        /// Italian
        ita,
        /// Japanese
        jpn,
        /// Georgian
        kat,
        /// Kazakh
        kaz,
        /// Korean
        kor,
        /// Latin
        lat,
        /// Latvian
        lav,
        /// Lithuanian
        lit,
        /// Ganda
        lug,
        /// Marathi
        mar,
        /// Macedonian
        mkd,
        /// Mongolian
        mon,
        /// Maori
        mri,
        /// Malay
        msa,
        /// Dutch
        nld,
        /// Norwegian Nynorsk
        nno,
        /// Norwegian Bokm√•l
        nob,
        /// Punjabi
        pan,
        /// Polish
        pol,
        /// Portuguese
        por,
        /// Romanian
        ron,
        /// Russian
        rus,
        /// Slovak
        slk,
        /// Slovene
        slv,
        /// Shona
        sna,
        /// Somali
        som,
        /// Sotho
        sot,
        /// Spanish
        spa,
        /// Serbian
        srp,
        /// Albanian
        sqi,
        /// Swahili
        swa,
        /// Swedish
        swe,
        /// Tamil
        tam,
        /// Telugu
        tel,
        /// Tagalog
        tgl,
        /// Thai
        tha,
        /// Tswana
        tsn,
        /// Tsonga
        tso,
        /// Turkish
        tur,
        /// Ukrainian
        ukr,
        /// Urdu
        urd,
        /// Vietnamese
        vie,
        /// Xhosa
        xho,
        /// Yoruba
        yor,
        /// Chinese
        zho,
        /// Zulu
        zul,
    }

    /// Select the detected language for the provided input based on the list of possible languages.
    /// If no language matches, None is returned.
    ///
    /// text: Text input
    /// languages: All languages that should be considered during detection.
    // @since(version = 0.3.0)
    record select-language-request {
        text: string,
        languages: list<language>,
    }

    // @since(version = 0.3.0)
    select-language: func(request: list<select-language-request>) -> list<option<language>>;

    /// Completion request parameters
    @since(version = 0.2.3)
    record completion-request {
        model: string,
        prompt: string,
        params: completion-params
    }

    /// @since(version = 0.3.0)
    complete: func(requests: list<completion-request>) -> list<completion>;

    /// Which documents you want to search in, and which type of index should be used
    @since(version = 0.2.6)
    record index-path {
        /// The namespace the collection belongs to
        namespace: string,
        /// The collection you want to search in
        collection: string,
        /// The search index you want to use for the collection
        index: string,
    }

    @since(version = 0.2.6)
    record document-path {
        namespace: string,
        collection: string,
        name: string,
    }

    @since(version = 0.2.6)
    record search-result {
        document-path: document-path,
        content: string,
        score: f64,
    }

    // @since(version = 0.3.0)
    record search-request {
        index-path: index-path,
        query: string,
        max-results: u32,
        min-score: option<f64>
    }

    // @since(version = 0.3.0)
    search: func(requests: list<search-request>) -> list<list<search-result>>;

    /// @since(version = 0.3.0)
    document-metadata: func(requests: list<document-path>) -> list<option<list<u8>>>;

    /// @since(version = 0.3.0)
    record document {
        path: document-path,
        contents: list<modality>,
        metadata: option<list<u8>>,
    }

    /// @since(version = 0.3.0)
    variant modality {
        text(string),
        /// We don't expose the image contents, as none of the models support multi-modal.
        image,
    }

    /// @since(version = 0.3.0)
    documents: func(requests: list<document-path>) -> list<document>;
}
