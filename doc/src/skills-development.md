# Developing and deploying Skills

## Developing Skills

A Skill is a WebAssembly Component compiled against our WIT (**W**ebAssembly **I**nterface **T**ypes) we call the **C**ognitive **S**ystem **I**nterface (CSI). The good thing is that you can compile almost any language into WebAssembly. Here we explain only how it works for Python.

## Cheatsheet

Just all the commands needed to setup your environment at a glance. Read the next chapter to understand them.

```shell
curl https://pharia-kernel.aleph-alpha.stackit.run/skill.wit > skill.wit
python -m venv .venv
source .venv/bin/activate
pip install componentize-py
componentize-py -d skill.wit -w skill bindings .
# ... write skill code in e.g. haiku.py ...
componentize-py -d skill.wit -w skill componentize haiku -o ./haiku.wasm
```

## Developing Skills in Python

This is a step-by-step instruction of how to write Skills. There is also an example repository at <https://gitlab.aleph-alpha.de/engineering/haiku-skill-python> for you to explore.

First you need to create the Python bindings for the Cognitive System Interface. In order to do so, you need a copy of `./skill.wit`. For the purpose of this documentation we assume the base url is `https://pharia-kernel.aleph-alpha.stackit.run`. Please replace it with your URL of Pharia Kernel hosted by your operations team. Pharia Kernel serves the most recent version at `{base_url}/skill.wit` so in our example it would be `https://pharia-kernel.aleph-alpha.stackit.run/skill.wit`. Just access the route and copy the contents as `skill.wit` in your development directory.

The current version looks like this:

```wit
package pharia:skill@0.2.3;

@since(version = 0.1.0)
world skill {
    @since(version = 0.1.0)
    import csi;
    @since(version = 0.1.0)
    export skill-handler;
}

@since(version = 0.1.0)
interface skill-handler {
    /// The set of errors which may be raised by functions in this interface
    @since(version = 0.1.0)
    variant error {
        internal(string),
        invalid-input(string)
    }

    @since(version = 0.1.0)
    run: func(input: list<u8>) -> result<list<u8>, error>;
}

// A WASI interface dedicated to interacting with Large Language Models and other AI-related tasks.
@since(version = 0.1.0)
interface csi {
    /// The reason the model finished generating
    @since(version = 0.1.0)
    enum finish-reason {
        /// The model hit a natural stopping point or a provided stop sequence
        stop,
        /// The maximum number of tokens specified in the request was reached
        length,
        /// Content was omitted due to a flag from content filters
        content-filter,
    }

    /// The result of a completion, including the text generated as well as
    /// why the model finished completing.
    @since(version = 0.1.0)
    record completion {
        /// The text generated by the model
        text: string,
        /// The reason the model finished generating
        finish-reason: finish-reason,
    }

    /// Completion request parameters
    @since(version = 0.2.0)
    record completion-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<float64>,
        /// The number of possible next tokens the model will choose from.
        top-k: option<u32>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<float64>,
        /// A list of sequences that, if encountered, the API will stop generating further tokens.
        stop: list<string>,
    }

    @since(version = 0.2.0)
    complete: func(model: string, prompt: string, params: completion-params) -> completion;

    /// Chunking parameters
    @since(version = 0.2.1)
    record chunk-params {
        /// The name of the model the chunk is intended to be used for.
        /// This must be a known model.
        model: string,
        /// The maximum number of tokens that should be returned per chunk.
        max-tokens: u32,
    }

    @since(version = 0.2.1)
    chunk: func(text: string, params: chunk-params) -> list<string>;

    /// ISO 639-3
    @since(version = 0.2.2)
    enum language {
        /// English
        eng,
        /// German
        deu,
    }

    /// Select the detected language for the provided input based on the list of possible languages.
    /// If no language matches, None is returned.
    ///
    /// text: Text input
    /// languages: All languages that should be considered during detection.
    @since(version = 0.2.2)
    select-language: func(text: string, languages: list<language>) -> option<language>;

    /// Completion request parameters
    @since(version = 0.2.3)
    record completion-request {
        model: string,
        prompt: string,
        params: completion-params
    }

    @since(version = 0.2.3)
    complete-all: func(requests: list<completion-request>) -> list<completion>;
}
```

To convert the `skill.wit` into bindings you need to install `componentize-py`. Choose the virtualization of your choice (conda, virtual env, ...) and pip install it.

```shell
pip install componentize-py
```

With `componentize-py` installed, we can now generate the bindings:

```shell
componentize-py -d skill.wit -w skill bindings .
```

This will create a `skill` module containing bindings to the Cognitive System Interface in the current directory. You can now import this module in your Python code. Here is an example of a Skill creating a haiku using the `skill` module.

```Python
import skill.exports
from skill.imports import csi
import json

class SkillHandler(skill.exports.SkillHandler):
    def run(self, input: bytes) -> bytes:
        input = json.loads(input)
        prompt = f"""Write a haiku about {in_}

### Response:"""
        params = csi.CompletionParams(100, None, None, None, [])
        completion = csi.complete( "luminous-extended-control", prompt, params)
        return json.dumps(completion.text).encode()
```

Now that we have written our Skill, we need to compile it into a component.

```shell
componentize-py -d skill.wit -w skill componentize haiku -o ./haiku.wasm
```

This will create a file called `haiku.wasm`, which can now be deployed into Pharia Kernel.

## Deploying Skills to the Kernel

In order to make a Skill available in Pharia Kernel two criteria need to be met:

* The skill must be deployed as a component to an OCI registry (for local development a directory might also suffice)
* The skill must be configured in the namespace configuration (a `toml` file, typically checked into a Git repository)

If your team does not own a namespace yet, you can request one from your Pharia Kernel operators. They will associate it with a `namespace.toml` which lists the skills you want to deploy as well as an OCI registry to load the skill code into. Ideally your team owns both, so you can deploy skills in self service.

The TOML file lists all skills in the namespace. Here is an example:

```toml
skills = [
    { name = "greet" },
    { name = "haiku", tag = "v1.0.1" },
]
```

With this configuration, Pharia Kernel looks in the associated OCI registry for two skills: the "greet" skill with the tag "latest" and the "haiku" skill with the tag "v1.0.1". If the configuration changes, Pharia Kernel will pick up the changes and reload the skills.

Skills are not containers. Yet, we still publish them into container repositories. Ask your administrator which ones are linked to your instance of Pharia Kernel. At Aleph Alpha we are currently using the registry `registry.gitlab.aleph-alpha.de` and the repository `engineering/pharia-skills/skills`. It is recommended to publish the Skill using the `pharia-skill` command line tool. `pharia-skill` is deployed as a container image to our Artifactory. You can acquire it with Podman like this.

**Attention:** We are currently still in a closed beta. Which means people outside of Aleph Alpha can not download our Pharia Kernel images. You may need to request a JFrog account, or extend the permission of your JFrog account to see images internal to the Aleph Alpha Organization. You can do so, by reaching out to us via the Product Service Desk:

* Service Desk: <https://aleph-alpha.atlassian.net/servicedesk/customer/portals>
* How to create a ticket: <https://aleph-alpha.atlassian.net/wiki/spaces/EN/pages/847937592/Aleph+Alpha+Service+Desk+-+How+To>

```shell
podman login alephalpha.jfrog.io/pharia-kernel-images -u $JFROG_USER -p $JFROG_PASSWORD
podman pull alephalpha.jfrog.io/pharia-kernel-images/pharia-skill:latest
podman tag alephalpha.jfrog.io/pharia-kernel-images/pharia-skill:latest pharia-skill
```

Feel free to use `docker` instead, if you are more familiar with that tooling.

With the tooling available we can now upload the Skill.

```shell
podman run -v ./haiku.wasm:/haiku.wasm pharia-skill publish -R registry.gitlab.aleph-alpha.de -r engineering/pharia-skills/skills -u DUMMY_USER_NAME -p $GITLAB_TOKEN ./haiku.wasm
```

With our Gitlab registry, any user name will work, as long as you use a token. You can generate a token on your profile page. It is important to give write privilege.

Congratulations! Your Skill is now deployed. You can now use the HTTP API to call it.

# Local Pharia Kernel setup

## Get Pharia Kernel image

1. Access JFrog Artifactory via token:
    * Login to [JFrog](https://alephalpha.jfrog.io/ui/login/)
    * Click on 'Edit Profile'
    * Click on 'Generate an Identity Token'

2. Pull Pharia Kernel image

```shell
podman login alephalpha.jfrog.io/pharia-kernel-images # login in interactive mode
podman login alephalpha.jfrog.io/pharia-kernel-images -u $JFROG_USER -p $JFROG_PASSWORD # login in non-interactive mode

podman pull alephalpha.jfrog.io/pharia-kernel-images/pharia-kernel:latest
podman tag alephalpha.jfrog.io/pharia-kernel-images/pharia-kernel:latest pharia-kernel
```

## Start Pharia Kernel container

In order to run Pharia Kernel, you need to provide a namespace configuration:

1. Create a `skills` folder

For local skill development you need a folder that serves as a skill registry to store all compiled skills.

```shell
    # create the local skills folder
    mkdir skills
```

All skills in this folder are exposed in the namespace "local".
Any changes in this folder will be picked up by the Pharia Kernel automatically.

2. Start the container:

```shell
    podman run \
        -v ./skills:/app/skills \
        -e AA_API_TOKEN=$AA_API_TOKEN \
        -e NAMESPACE_UPDATE_INTERVAL=1 \
        -e LOG_LEVEL=debug \
        -p 8081:8081 \
        pharia-kernel
```

You can view the Pharia-Kernel's API documentation at <http://127.0.0.1:8081/api-docs>

## Build your skill

1. Set up a virtual python environment:

```shell
    # fetch the wit world
    curl http://127.0.0.1:8081/skill.wit > skill.wit

    # create python venv
    python -m venv .venv
    source .venv/bin/activate
    pip install componentize-py

    # create the wit bindings
    componentize-py -d skill.wit -w skill bindings .
```

2. Write your skill `my_skill.py`:

    a. Simple 'hello world' example:

    ```python
    import skill.exports
    import json

    class SkillHandler(skill.exports.SkillHandler):
        def run(self, input: bytes) -> bytes:
            input = json.loads(input)
            return json.dumps("Hello " + input).encode()
    ```

    b. Example that uses the CSI:

    ```python
        import skill.exports
        from skill.imports import csi
        import json

        class SkillHandler(skill.exports.SkillHandler):
            def run(self, input: bytes) -> bytes:
                input = json.loads(input)
                prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

        Cutting Knowledge Date: December 2023
        Today Date: 23 Jul 2024

        You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

        Provide a nice greeting for the person named: {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""
                params = csi.CompletionParams(10, None, None, None, [])
                completion = csi.complete( "llama-3.1-8b-instruct", prompt, params)
                return json.dumps(completion.text).encode()
    ```

3. Compile your skill:

```shell
    componentize-py -d skill.wit -w skill componentize my_skill -o ./skills/my_skill.wasm
```

4. Execute your skill:

```shell
    curl -v -X POST 127.0.0.1:8081/execute_skill \
        -H "Authorization: Bearer $AA_API_TOKEN" \
        -H 'Content-Type: application/json' \
        -d '{"skill":"local/my_skill", "input":"Homer"}'
```

5. Iterate

Whenever you change the skill code, you have to compile it again and you have to invalidate the Pharia Kernel's skill cache
in order to load the new skill version.

```shell
    # compile the skill
    componentize-py -d skill.wit -w skill componentize my_skill -o ./skills/my_skill.wasm

    # invalidate the cached version of 'my_skill'
    curl -v -X DELETE 127.0.0.1:8081/cached_skills/local%2fmy_skill
```

# Monitoring skill execution

You can monitor you skill by connecting the Pharia Kernel to an OpenTelemetry collector, e.g. Jaeger:

```shell
    podman run -d -p 4317:4317 -p 16686:16686 jaegertracing/all-in-one
```

Specify the collector endpoint via the environment variable `OPEN_TELEMETRY_ENDPOINT`:

```shell
    podman run \
        -v ./skills:/app/skills \
        -e AA_API_TOKEN=$AA_API_TOKEN \
        -e NAMESPACE_UPDATE_INTERVAL=1 \
        -e OPEN_TELEMETRY_ENDPOINT=http://host.containers.internal:4317 \
        -p 8081:8081 \
        pharia-kernel
```

You can view the monitoring at your [local Jaeger instance](http://localhost:16686)
